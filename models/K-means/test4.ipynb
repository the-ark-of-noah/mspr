{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:35:44.821571Z",
     "start_time": "2024-03-23T16:24:51.953374Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/t_t93b0n0wx8m9fcms_xg6640000gn/T/ipykernel_74771/2337602903.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_demographie = pd.read_sql(query_demographie, conn)\n",
      "/var/folders/00/t_t93b0n0wx8m9fcms_xg6640000gn/T/ipykernel_74771/2337602903.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_economie = pd.read_sql(query_economie, conn)\n",
      "/var/folders/00/t_t93b0n0wx8m9fcms_xg6640000gn/T/ipykernel_74771/2337602903.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_election = pd.read_sql(query_election, conn)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 96\u001B[0m\n\u001B[1;32m     94\u001B[0m     kmeans \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39mn_clusters, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m     95\u001B[0m     cluster_labels \u001B[38;5;241m=\u001B[39m kmeans\u001B[38;5;241m.\u001B[39mfit_predict(X_pca)\n\u001B[0;32m---> 96\u001B[0m     silhouette_avg \u001B[38;5;241m=\u001B[39m silhouette_score(X_pca, cluster_labels)\n\u001B[1;32m     97\u001B[0m     silhouette_scores\u001B[38;5;241m.\u001B[39mappend(silhouette_avg)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# Affichage du score de silhouette pour chaque nombre de clusters\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:140\u001B[0m, in \u001B[0;36msilhouette_score\u001B[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001B[0m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    139\u001B[0m         X, labels \u001B[38;5;241m=\u001B[39m X[indices], labels[indices]\n\u001B[0;32m--> 140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(silhouette_samples(X, labels, metric\u001B[38;5;241m=\u001B[39mmetric, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[1;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:292\u001B[0m, in \u001B[0;36msilhouette_samples\u001B[0;34m(X, labels, metric, **kwds)\u001B[0m\n\u001B[1;32m    288\u001B[0m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetric\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m metric\n\u001B[1;32m    289\u001B[0m reduce_func \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mpartial(\n\u001B[1;32m    290\u001B[0m     _silhouette_reduce, labels\u001B[38;5;241m=\u001B[39mlabels, label_freqs\u001B[38;5;241m=\u001B[39mlabel_freqs\n\u001B[1;32m    291\u001B[0m )\n\u001B[0;32m--> 292\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mpairwise_distances_chunked(X, reduce_func\u001B[38;5;241m=\u001B[39mreduce_func, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n\u001B[1;32m    293\u001B[0m intra_clust_dists, inter_clust_dists \u001B[38;5;241m=\u001B[39m results\n\u001B[1;32m    294\u001B[0m intra_clust_dists \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(intra_clust_dists)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2153\u001B[0m, in \u001B[0;36mpairwise_distances_chunked\u001B[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001B[0m\n\u001B[1;32m   2151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reduce_func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2152\u001B[0m     chunk_size \u001B[38;5;241m=\u001B[39m D_chunk\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m-> 2153\u001B[0m     D_chunk \u001B[38;5;241m=\u001B[39m reduce_func(D_chunk, sl\u001B[38;5;241m.\u001B[39mstart)\n\u001B[1;32m   2154\u001B[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001B[1;32m   2155\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m D_chunk\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:181\u001B[0m, in \u001B[0;36m_silhouette_reduce\u001B[0;34m(D_chunk, start, labels, label_freqs)\u001B[0m\n\u001B[1;32m    179\u001B[0m         sample_weights \u001B[38;5;241m=\u001B[39m D_chunk[i]\n\u001B[1;32m    180\u001B[0m         sample_labels \u001B[38;5;241m=\u001B[39m labels\n\u001B[0;32m--> 181\u001B[0m         cluster_distances[i] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mbincount(\n\u001B[1;32m    182\u001B[0m             sample_labels, weights\u001B[38;5;241m=\u001B[39msample_weights, minlength\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(label_freqs)\n\u001B[1;32m    183\u001B[0m         )\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m# intra_index selects intra-cluster distances within cluster_distances\u001B[39;00m\n\u001B[1;32m    186\u001B[0m end \u001B[38;5;241m=\u001B[39m start \u001B[38;5;241m+\u001B[39m n_chunk_samples\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from models.id_to_party_dict import id_to_party_dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Paramètres de connexion à la base de données PostgreSQL\n",
    "conn_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"15432\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\"\n",
    "}\n",
    "\n",
    "# Connexion à la base de données\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "\n",
    "# Requêtes SQL pour récupérer les données nécessaires\n",
    "query_demographie = \"SELECT * FROM demographie;\"\n",
    "query_economie = \"SELECT * FROM economie;\"\n",
    "query_election = \"SELECT * FROM election_2022_t1;\"\n",
    "\n",
    "# query_election = \"\"\"\n",
    "#     SELECT \"Parti 1\", \"% Voix/Exp 1\", \"Parti 2\", \"% Voix/Exp 2\", \"Parti 3\", \"% Voix/Exp 3\", \n",
    "#             \"Parti 4\", \"% Voix/Exp 4\", \"Parti 5\", \"% Voix/Exp 5\", \"Parti 6\", \"% Voix/Exp 6\", \n",
    "#             \"Parti 7\", \"% Voix/Exp 7\", \"Parti 8\", \"% Voix/Exp 8\", \"Parti 9\", \"% Voix/Exp 9\", \n",
    "#             \"Parti 10\", \"% Voix/Exp 10\", \"Parti 11\", \"% Voix/Exp 11\", \"Parti 12\", \"% Voix/Exp 12\"\n",
    "#      FROM election_2022_t1\n",
    "# \"\"\"\n",
    "# df_election = pd.read_sql(query_election, conn)\n",
    "\n",
    "# Chargement des données\n",
    "df_demographie = pd.read_sql(query_demographie, conn)\n",
    "df_economie = pd.read_sql(query_economie, conn)\n",
    "df_election = pd.read_sql(query_election, conn)\n",
    "\n",
    "# print(df_demographie.columns)\n",
    "# print(df_economie.columns)\n",
    "# print(df_election.columns)\n",
    "\n",
    "# Fermeture de la connexion à la base de données\n",
    "conn.close()\n",
    "\n",
    "# Mapper les noms des partis politiques aux identifiants numériques dans df_election\n",
    "\n",
    "party_to_id_dict = {v: k for k, v in id_to_party_dict.items()}\n",
    "\n",
    "# Mapper les noms des partis politiques aux identifiants numériques dans df_election\n",
    "for i in range(1, 13):\n",
    "    df_election[f\"Parti {i}\"] = df_election[f\"Parti {i}\"].map(party_to_id_dict)\n",
    "\n",
    "# Fusion des DataFrames\n",
    "df_merged = pd.merge(pd.merge(df_demographie, df_economie, on='code_postal'), df_election, on='code_postal')\n",
    "\n",
    "# Sélection des caractéristiques pour le clustering\n",
    "features_to_include = [\n",
    "    '65-99_2020', '60-99_2023', '25-59_2023', '0-24_2023',\n",
    "    'dens_pop', 'avg_2023', 'Part des minima sociaux dans le rev. disp. 2021',\n",
    "    'Part des prestations familiales dans le rev. disp. 2021', 'Médiane du niveau de vie 2021',\n",
    "    '% Voix/Ins 1', '% Voix/Ins 2', '% Voix/Ins 3', '% Voix/Ins 4', '% Voix/Ins 5',\n",
    "    '% Voix/Ins 6', '% Voix/Ins 7', '% Voix/Ins 8', '% Voix/Ins 9',\n",
    "    '% Voix/Ins 10', '% Voix/Ins 11', '% Voix/Ins 12', '% Voix/Exp 1',\n",
    "    '% Voix/Exp 2', '% Voix/Exp 3', '% Voix/Exp 4', '% Voix/Exp 5',\n",
    "    '% Voix/Exp 6', '% Voix/Exp 7', '% Voix/Exp 8', '% Voix/Exp 9',\n",
    "    '% Voix/Exp 10', '% Voix/Exp 11', '% Voix/Exp 12'\n",
    "]\n",
    "\n",
    "# Imputation des valeurs manquantes et standardisation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(df_merged[features_to_include])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Réduction de dimension avec PCA\n",
    "pca = PCA(n_components=0.95)  # Conserver 95% de la variance expliquée\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Optimisation du nombre de clusters avec le score de silhouette\n",
    "silhouette_scores = []\n",
    "range_clusters = range(2, 12)\n",
    "for n_clusters in range_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_pca)\n",
    "    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Affichage du score de silhouette pour chaque nombre de clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_clusters, silhouette_scores, marker='o')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('Score de silhouette')\n",
    "plt.title('Optimisation du nombre de clusters')\n",
    "plt.show()\n",
    "\n",
    "# Sélection et application du nombre optimal de clusters\n",
    "optimal_clusters = np.argmax(silhouette_scores) + 2\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "df_merged['cluster'] = kmeans_optimal.fit_predict(X_pca)\n",
    "\n",
    "# À ce stade, vous pouvez continuer avec votre analyse de clusters ou avec une modélisation prédictive spécifique\n",
    "# en fonction des clusters identifiés, comme illustré précédemment.\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "# X = df_merged.drop(columns=['cluster'])\n",
    "# y = df_merged['cluster']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# \n",
    "# # Entraînement d'un classificateur RandomForest\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# y_pred = rf.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Temps d'exécution du script: {elapsed_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e9016baef17bb2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

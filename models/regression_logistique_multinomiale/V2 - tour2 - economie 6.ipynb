{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T11:16:01.436534Z",
     "start_time": "2024-03-23T11:15:50.555784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/lnzs0dyx0k1g2nvs92hf0syr0000gn/T/ipykernel_8233/1456310199.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_election = pd.read_sql(query_election, conn)\n",
      "/var/folders/9z/lnzs0dyx0k1g2nvs92hf0syr0000gn/T/ipykernel_8233/1456310199.py:70: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_economie = pd.read_sql(query_economie, conn_economie)\n",
      "/var/folders/9z/lnzs0dyx0k1g2nvs92hf0syr0000gn/T/ipykernel_8233/1456310199.py:85: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_securite = pd.read_sql(query_securite, conn_securite)\n",
      "/var/folders/9z/lnzs0dyx0k1g2nvs92hf0syr0000gn/T/ipykernel_8233/1456310199.py:100: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_densite = pd.read_sql(query_densite, conn_economie)\n",
      "/var/folders/9z/lnzs0dyx0k1g2nvs92hf0syr0000gn/T/ipykernel_8233/1456310199.py:115: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_social = pd.read_sql(query_social, conn_social)\n",
      "/var/folders/9z/lnzs0dyx0k1g2nvs92hf0syr0000gn/T/ipykernel_8233/1456310199.py:134: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_election[\"winner\"].fillna(-1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle sur l'ensemble d'entraînement: 0.6433249562104942\n",
      "Précision du modèle sur les données de l'Yonne: 0.5849372384937238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erwanbuisson/Library/Mobile Documents/com~apple~CloudDocs/Documents/Interpreteur Python/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dictionnaire de correspondance entre les noms des partis politiques et leurs identifiants numériques\n",
    "id_to_party_dict = {\n",
    "    1: \"Lutte Ouvrière\",\n",
    "    2: \"Parti Communiste Français\",\n",
    "    3: \"La République En Marche\",\n",
    "    4: \"Résistons\",\n",
    "    5: \"Rassemblement National\",\n",
    "    6: \"Reconquête\",\n",
    "    7: \"La France Insoumise\",\n",
    "    8: \"Parti Socialiste\",\n",
    "    9: \"Europe Écologie Les Verts\",\n",
    "    10: \"Les Républicains\",\n",
    "    11: \"Nouveau Parti Anticapitaliste\",\n",
    "    12: \"Debout La France\"\n",
    "}\n",
    "\n",
    "# Paramètres de connexion à la base de données PostgreSQL pour les élections\n",
    "conn_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"15432\", \n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\"\n",
    "}\n",
    "\n",
    "# Paramètres de connexion à la base de données PostgreSQL pour les données économiques\n",
    "conn_params_economie = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"15432\", \n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\"\n",
    "}\n",
    "\n",
    "# Connexion à la base de données pour les élections\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "\n",
    "# Requête SQL pour sélectionner les données de la table election_2022_t1\n",
    "query_election = \"\"\"\n",
    "    SELECT winner, code_postal, \"Libellé de la commune\"\n",
    "    FROM election_2022_t1\n",
    "\"\"\"\n",
    "df_election = pd.read_sql(query_election, conn)\n",
    "\n",
    "# Fermeture de la connexion\n",
    "conn.close()\n",
    "\n",
    "# Connexion à la base de données pour les données économiques\n",
    "conn_economie = psycopg2.connect(**conn_params_economie)\n",
    "\n",
    "# Requête SQL pour sélectionner les données économiques de la table economie\n",
    "query_economie = \"\"\"\n",
    "    SELECT avg_1982, avg_1983, avg_1984, avg_1985, avg_1986, avg_1987, avg_1988, avg_1989,\n",
    "           avg_1990, avg_1991, avg_1992, avg_1993, avg_1994, avg_1995, avg_1996, avg_1997,\n",
    "           avg_1998, avg_1999, avg_2000, avg_2001, avg_2002, avg_2003, avg_2004, avg_2005,\n",
    "           avg_2006, avg_2007, avg_2008, avg_2009, avg_2010, avg_2011, avg_2012, avg_2013,\n",
    "           avg_2014, avg_2015, avg_2016, avg_2017, avg_2018, avg_2019, avg_2020, avg_2021,\n",
    "           avg_2022, avg_2023, code_postal\n",
    "    FROM economie\n",
    "\"\"\"\n",
    "\n",
    "# Lire les données économiques depuis la base de données\n",
    "df_economie = pd.read_sql(query_economie, conn_economie)\n",
    "\n",
    "# Fermeture de la connexion\n",
    "conn_economie.close()\n",
    "\n",
    "# Connexion à la base de données pour les données de sécurité\n",
    "conn_securite = psycopg2.connect(**conn_params)\n",
    "\n",
    "# Requête SQL pour sélectionner les données de la table \"securite\"\n",
    "query_securite = \"\"\"\n",
    "    SELECT code_postal, tauxpourcent\n",
    "    FROM securite\n",
    "\"\"\"\n",
    "\n",
    "# Lire les données de la table \"securite\" depuis la base de données\n",
    "df_securite = pd.read_sql(query_securite, conn_securite)\n",
    "\n",
    "# Fermeture de la connexion\n",
    "conn_securite.close()\n",
    "\n",
    "# Connexion à la base de données pour les données économiques\n",
    "conn_economie = psycopg2.connect(**conn_params_economie)\n",
    "\n",
    "# Requête SQL pour sélectionner les données économiques de la table economie\n",
    "query_densite = \"\"\"\n",
    "    SELECT code_postal, dens_pop\n",
    "    FROM demographie\n",
    "\"\"\"\n",
    "\n",
    "# Lire les données économiques depuis la base de données\n",
    "df_densite = pd.read_sql(query_densite, conn_economie)\n",
    "\n",
    "# Fermeture de la connexion\n",
    "conn_economie.close()\n",
    "\n",
    "# Connexion à la base de données PostgreSQL pour la table \"social\"\n",
    "conn_social = psycopg2.connect(**conn_params)\n",
    "\n",
    "query_social = \"\"\"\n",
    "    SELECT code_postal, taux \n",
    "    FROM social \n",
    "    WHERE indicateur = 'Part des diplômés d''un BAC+5 ou plus dans la population non scolarisée';\n",
    "\"\"\"\n",
    "\n",
    "# Lire les données de la table \"social\" depuis la base de données\n",
    "df_social = pd.read_sql(query_social, conn_social)\n",
    "\n",
    "# Fermeture de la connexion\n",
    "conn_social.close()\n",
    "\n",
    "# Continuez avec le reste de votre code utilisant df_social.\n",
    "\n",
    "\n",
    "# Calculer la moyenne du taux de pourcentage de sécurité pour chaque département\n",
    "df_densite = df_densite.groupby('code_postal')['dens_pop'].mean().reset_index()\n",
    "\n",
    "# Calculer la moyenne du taux de pourcentage de sécurité pour chaque département\n",
    "df_securite = df_securite.groupby('code_postal')['tauxpourcent'].mean().reset_index()\n",
    "\n",
    "# Mapper les noms des partis politiques aux identifiants numériques\n",
    "party_to_id_dict = {v: k for k, v in id_to_party_dict.items()}\n",
    "df_election[\"winner\"] = df_election[\"winner\"].map(party_to_id_dict)\n",
    "\n",
    "# Remplacer les valeurs manquantes dans la colonne \"winner\" par une valeur arbitraire\n",
    "df_election[\"winner\"].fillna(-1, inplace=True)\n",
    "\n",
    "# Convertir la colonne \"winner\" en float\n",
    "df_election[\"winner\"] = df_election[\"winner\"].astype(float)\n",
    "\n",
    "# Encodage des variables catégorielles\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fusionner d'abord les DataFrames df_election et df_economie sur la colonne code_postal\n",
    "df_merged = pd.merge(df_election, df_economie, on=\"code_postal\", how=\"left\")\n",
    "\n",
    "# Ensuite, fusionner le résultat avec df_securite également sur la colonne code_postal\n",
    "df_merged = pd.merge(df_merged, df_securite, on=\"code_postal\", how=\"left\")\n",
    "\n",
    "# Ensuite, fusionner le résultat avec df_securite également sur la colonne code_postal\n",
    "df_merged = pd.merge(df_merged, df_densite, on=\"code_postal\", how=\"left\")\n",
    "\n",
    "# Ensuite, fusionner le résultat avec df_securite également sur la colonne code_postal\n",
    "df_merged = pd.merge(df_merged, df_social, on=\"code_postal\", how=\"left\")\n",
    "\n",
    "# Supprimer les lignes contenant des valeurs NaN de df_merged\n",
    "df_merged.dropna(inplace=True)\n",
    "\n",
    "# Calculer la moyenne du taux de pourcentage de sécurité pour chaque département\n",
    "df_department_avg = df_merged.groupby('code_postal')['tauxpourcent'].mean().reset_index()\n",
    "\n",
    "# Exclure les données du département de l'Yonne (89) du jeu d'entraînement\n",
    "df_train = df_merged[df_merged[\"code_postal\"] != 89]\n",
    "\n",
    "# Sélectionner uniquement les données du département de l'Yonne (89) pour les tests\n",
    "df_test_yonne = df_merged[df_merged[\"code_postal\"] == 89]\n",
    "\n",
    "# Ajouter 10 autres départements au département de l'Yonne (89) pour les tests\n",
    "other_departments = [90, 91, 92, 93, 94, 95, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "for department in other_departments:\n",
    "    df_test_yonne = pd.concat([df_test_yonne, df_merged[df_merged[\"code_postal\"] == department]])\n",
    "\n",
    "# Exclure les données des départements ajoutés des données d'entraînement\n",
    "df_train = df_train[~df_train[\"code_postal\"].isin(other_departments)]\n",
    "\n",
    "# Séparation des features et de la target pour l'ensemble d'entraînement\n",
    "X_train = df_train.drop(columns=[\"Libellé de la commune\", \"winner\"])\n",
    "y_train = df_train[\"winner\"]\n",
    "\n",
    "# Séparation des features et de la target pour l'ensemble de test de l'Yonne\n",
    "X_test_yonne = df_test_yonne.drop(columns=[\"Libellé de la commune\", \"winner\"])\n",
    "y_test_yonne = df_test_yonne[\"winner\"]\n",
    "\n",
    "# Entraînement du modèle de régression logistique multinomiale\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble d'entraînement\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# Évaluation de la précision du modèle sur l'ensemble d'entraînement\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Précision du modèle sur l'ensemble d'entraînement:\", accuracy_train)\n",
    "\n",
    "# Prédiction sur les données de test de l'Yonne\n",
    "y_pred_yonne = model.predict(X_test_yonne)\n",
    "\n",
    "# Évaluation de la précision du modèle sur les données de l'Yonne\n",
    "accuracy_yonne = accuracy_score(y_test_yonne, y_pred_yonne)\n",
    "print(\"Précision du modèle sur les données de l'Yonne:\", accuracy_yonne)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T14:51:52.005337Z",
     "start_time": "2024-03-18T14:51:52.005297Z"
    }
   },
   "id": "e52a49289a74df71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
